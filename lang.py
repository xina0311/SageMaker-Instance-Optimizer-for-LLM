def set_lang():
    return {
        "sidebar_title": "Model Parameters",
        "model": "Model Name",
        "model_desc": "Name of the model being used",
        "params": "Number of parameters (in billions)",
        "params_desc": "Total number of parameters in the model, in billions",
        "precision": "Precision",
        "precision_desc": "Numerical precision used for model weights and calculations",
        "batch_size": "Batch Size",
        "batch_size_desc": "Number of samples processed in one forward/backward pass",
        "seq_len": "Sequence Length",
        "seq_len_desc": "Maximum length of input sequences",
        "hidden_size": "Hidden Size",
        "hidden_size_desc": "Dimension of the model's hidden layers",
        "num_layers": "Number of Layers",
        "num_layers_desc": "Total number of layers in the model",
        "num_heads": "Number of Attention Heads",
        "num_heads_desc": "Number of attention heads in each layer",
        "optional_params": "Optional Parameters for Estimating Training Memory",
        "total_inference": "Total Inference Memory (with 20% overhead)",
        "total_training": "Total Training Memory",
        "calc_details": "Calculation details:",
        "model_mem": "Model Memory",
        "act_mem": "Activation Memory",
        "opt_mem": "Optimizer Memory",
        "grad_mem": "Gradient Memory",
        "total_mem": "Total memory",
        "formulas": "Formulas Used",
        "inf_mem": "Inference Memory",
        "train_mem": "Training Memory",
        "where": "Where:",
        "bytes_per_param": "Where **bytes per param**:",
        "optimizer": "Optimizer (for training)",
        "notes": "Notes:",
        "note1": "This is a simplified estimation and may not account for all factors affecting GPU memory usage.",
        "note2": "Actual memory usage may vary depending on the specific model architecture and implementation.",
        "note3": "For training, you may need additional memory for optimizer states and gradients.",
        "note4": "The activation memory calculation uses a full recomputation approach for training, which is an approximation.",
        "note5": "Int4 and Int8 precisions are approximations and may not be supported by all hardware or frameworks.",
        "note6": "For int4 and int8, we assume the same gradient memory as mixed precision for training, which might not be accurate for all implementations.",
        "note7": "The inference memory calculation is a rough estimate and may not be as accurate for very large models or extreme batch sizes.",
        "reference": "Reference:",
        "suitable_instances": "Suitable SageMaker GPU Instances:",
    }
